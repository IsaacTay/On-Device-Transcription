{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ODT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VVfIT5qvDxL8",
        "outputId": "68364d91-2034-4a09-d519-30986ade1467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  # Notebook specific code\n",
        "\n",
        "  %matplotlib inline\n",
        "  %tensorflow_version 2.x \n",
        "  # !pip install tensorflow-gpu==2.0.0-beta1 -q\n",
        "\n",
        "  try:\n",
        "    import tensorflow as tf\n",
        "    import warprnnt_tensorflow\n",
        "  except:\n",
        "    !git clone https://github.com/HawkAaron/warp-transducer\n",
        "    !export CUDA_HOME=/usr/local/cuda; cd warp-transducer; mkdir build; cd build; cmake -DCUDA_TOOLKIT_ROOT_DIR=$CUDA_HOME -DCMAKE_CXX_FLAGS=-D_GLIBCXX_USE_CXX11_ABI=0 ..; make\n",
        "    !export CUDA_HOME=/usr/local/cuda; cd /content/warp-transducer/tensorflow_binding; python setup.py install\n",
        "    !rm -r warp-transducer\n",
        "\n",
        "    import os\n",
        "    os.kill(os.getpid(), 9)\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgI8WSyIfjtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import warprnnt_tensorflow\n",
        "\n",
        "def gpu_config(mem_limit=None, print_gpus=True, log_device_placement=False):\n",
        "  tf.debugging.set_log_device_placement(log_device_placement)\n",
        "\n",
        "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "  if print_gpus:\n",
        "    print(gpus)\n",
        "  if mem_limit is not None and gpus:\n",
        "    try:\n",
        "      tf.config.experimental.set_virtual_device_configuration(\n",
        "            gpus[0],\n",
        "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*10)])\n",
        "    except RuntimeError as e:\n",
        "      # Virtual devices must be set before GPUs have been initialized\n",
        "      print(e)\n",
        "      \n",
        "gpu_config(1024*12, print_gpus=True, log_device_placement=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36MJnQH4JQfY",
        "colab_type": "text"
      },
      "source": [
        "This is the code for the On Device Transcription Model, based of [this blog post](https://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7Gq6Gk0PQf8",
        "colab_type": "text"
      },
      "source": [
        "Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWOzMUKuADMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import urllib.request\n",
        "import shutil\n",
        "\n",
        "import tarfile\n",
        "\n",
        "def download_data():\n",
        "  url = \"http://www.openslr.org/resources/60/dev-clean.tar.gz\"\n",
        "  file_name = \"dev-clean.tar.gz\"\n",
        "  dir_name = \"LibriTTS/dev-clean\"\n",
        "\n",
        "  if not os.path.isdir(dir_name):\n",
        "    with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
        "      shutil.copyfileobj(response, out_file)\n",
        "    tar = tarfile.open(file_name)\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "    \n",
        "  files = []\n",
        "  for (dirpath, dirnames, filenames) in os.walk(dir_name):\n",
        "      for filename in filenames:\n",
        "        filepath = dirpath + \"/\" + filename\n",
        "        if filepath.endswith(\".wav\"):\n",
        "          files.append(filepath[:-4])\n",
        "  \n",
        "  return files\n",
        "\n",
        "files = download_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL2oITPZXB9c",
        "colab_type": "text"
      },
      "source": [
        "Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag-7WqgucSTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "labels = [''] + [chr(i) for i in range(ord('a'), ord('z') + 1)] + [' '] + list(string.digits + string.punctuation)\n",
        "label_lookup = {}\n",
        "for i in range(len(labels)):\n",
        "  label_lookup[labels[i]] = i\n",
        "label_size = len(labels)\n",
        "\n",
        "def text_normalise(text_data):\n",
        "   \n",
        "  labels = [label_lookup[x] for x in text_data.lower()]\n",
        "  \n",
        "  return labels\n",
        "\n",
        "import librosa, librosa.display\n",
        "import numpy as np\n",
        "\n",
        "def get_data(filepath, sr=24000):\n",
        "  filepath = bytes.decode(filepath)\n",
        "  y, _ = librosa.load(filepath + \".wav\", mono=True, sr=sr)\n",
        "  \n",
        "  y = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=int(25*sr/1000), hop_length=int(10*sr/1000), n_mels=80)\n",
        "  y = librosa.power_to_db(y, ref=np.max)\n",
        "  \n",
        "  with open(filepath + \".original.txt\", 'r') as f:\n",
        "    text = f.read()\n",
        "  return y.astype(np.float32), text_normalise(text), len(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEaj4JLUOMCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset(audio_batches, text_batches, rnnt_batches, caching=False):\n",
        "  @tf.function\n",
        "  def data_shapping(audio, text, text_length):\n",
        "    text = tf.cast(text, tf.int32)\n",
        "    text_length = tf.cast(text_length, tf.int32)\n",
        "\n",
        "    audio_shape = tf.shape(audio)\n",
        "    audio = tf.pad(audio, [[0, 0], [0, 3 - tf.math.floormod(audio_shape[1], 3)]], constant_values = -80.0)\n",
        "    audio_shape = tf.shape(audio)\n",
        "    audio = tf.reshape(tf.transpose(audio), (audio_shape[1]//3, 3 * audio_shape[0]))\n",
        "    return audio, tf.one_hot(tf.concat([[0], text], axis=-1), label_size, dtype=tf.float32), tf.reshape(tf.concat([[tf.cast(tf.shape(audio)[0], tf.int32)], [text_length], text], axis=-1), (-1, 1))\n",
        "    #(audio, text_embedded, [audio_length, text_length, text])\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(files)\n",
        "  dataset = dataset.shuffle(buffer_size=10000)\n",
        "\n",
        "  # dataset to be both\n",
        "  dataset = dataset.map(lambda x: tf.numpy_function(get_data, [x], [tf.float32, tf.int64, tf.int64]))\n",
        "  dataset = dataset.map(data_shapping, 16)\n",
        "  if caching: dataset.cache()\n",
        "\n",
        "  @tf.function\n",
        "  def audio_crop(audio, text_embedded, text_and_length):\n",
        "    return audio, text_and_length\n",
        "\n",
        "  # Split audio\n",
        "  audio_dataset = dataset.map(audio_crop, 16)\n",
        "\n",
        "  @tf.function\n",
        "  def text_crop(audio, text_embedded, text_and_length):\n",
        "    return text_embedded[:-1], text_embedded[1:]\n",
        "\n",
        "  # Split text\n",
        "  text_dataset = dataset.map(text_crop, 16)\n",
        "\n",
        "  # Test dataset split\n",
        "\n",
        "  test_dataset = dataset.take(1000)\n",
        "\n",
        "  # Batching split datasets\n",
        "\n",
        "  text_dataset = text_dataset.padded_batch(text_batches, padded_shapes=((512, label_size), (512, label_size))).prefetch(20)\n",
        "  audio_dataset = audio_dataset.padded_batch(audio_batches, padded_shapes=((1024, 80*3), (512, 1)), drop_remainder=True).prefetch(20)\n",
        "\n",
        "  # Batching RNNT train & test datasets\n",
        "\n",
        "  @tf.function\n",
        "  def data_grouping(audio, text_embedded, targets):\n",
        "    return ((audio, text_embedded), targets)\n",
        "\n",
        "  def dataset_shapping(dataset, batches=1):\n",
        "  # dataset = dataset.padded_batch(1, padded_shapes=((None, 80*3), (None, ), (), ())).prefetch(2) # 10 loops, best of 3: 393 ms per loop\n",
        "    dataset = dataset.padded_batch(batches, padded_shapes=((1024, 80*3), (512, label_size), (512, 1))).prefetch(20) # 10 loops, best of 3: 461 ms per loop\n",
        "\n",
        "    dataset = dataset.map(data_grouping, 2)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  dataset = dataset_shapping(dataset, rnnt_batches)\n",
        "  test_dataset = dataset_shapping(test_dataset, 1).repeat()\n",
        "  \n",
        "  return audio_dataset, text_dataset, dataset, test_dataset\n",
        "\n",
        "audio_dataset, text_dataset, dataset, test_dataset = get_dataset(audio_batches=8, text_batches=64, rnnt_batches=4, caching=False)\n",
        "\n",
        "# next(iter(text_dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufnq3elmz1mc",
        "colab_type": "text"
      },
      "source": [
        "Max frame size: 1296"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOzQlDvq2qJI",
        "colab_type": "text"
      },
      "source": [
        "Max text length: 419"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWhwGOunTZIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "class Sequence():\n",
        "  def __init__(self, seq=None, hidden=None, blank=0):\n",
        "    if seq is None:\n",
        "      self.g = [] # predictions of phoneme language model\n",
        "      self.k = [blank] # prediction phoneme label\n",
        "      self.orig = []\n",
        "      # self.h = [None] # input hidden vector to phoneme model\n",
        "      self.h = hidden\n",
        "      self.logp = 0 # probability of this sequence, in log scale\n",
        "    else:\n",
        "      self.g = seq.g[:] # save for prefixsum\n",
        "      self.k = seq.k[:]\n",
        "      self.orig = seq.orig[:]\n",
        "      self.h = seq.h\n",
        "      self.logp = seq.logp\n",
        "  \n",
        "  def resetOrig(self):\n",
        "    self.orig = self.k\n",
        "\n",
        "  def __str__(self):\n",
        "    return 'Prediction: {} ({:.2f})'.format(self.string(), -self.logp)\n",
        "  \n",
        "  def string(self):\n",
        "    return ''.join([labels[i] for i in self.k])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj1XR1bwH9fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tqdm\n",
        "\n",
        "def AudioModel():\n",
        "  audio_model = tf.keras.Sequential()\n",
        "  audio_model.add(tf.keras.layers.Masking(mask_value=0.))\n",
        "  for i in range(8):\n",
        "    audio_model.add(tf.keras.layers.LSTM(1024,#2048,\n",
        "                                         return_sequences=True))#, recurrent_activation='hard_sigmoid'))\n",
        "    audio_model.add(tf.keras.layers.LayerNormalization(2))\n",
        "  \n",
        "  return audio_model\n",
        "\n",
        "def TextModel():\n",
        "  text_model = tf.keras.Sequential()\n",
        "  text_model.add(tf.keras.layers.Masking(mask_value=0.))\n",
        "  for _ in range(5):\n",
        "    text_model.add(tf.keras.layers.LSTM(1024,\n",
        "                                        return_sequences=True))#, recurrent_activation='hard_sigmoid'))\n",
        "    text_model.add(tf.keras.layers.LayerNormalization(2))\n",
        "    \n",
        "  return text_model\n",
        "\n",
        "class EncapsulationModel(tf.keras.Model):\n",
        "  def __init__(self, sub_model, output_units, softmax=True):\n",
        "    super(EncapsulationModel, self).__init__()\n",
        "    \n",
        "    self.model = tf.keras.Sequential()\n",
        "    self.model.add(sub_model)\n",
        "    for _ in range(2):\n",
        "      self.model.add(tf.keras.layers.LSTM(output_units,\n",
        "                                         return_sequences=True))\n",
        "    self.softmax = softmax\n",
        "    \n",
        "  @tf.function\n",
        "  def call(self, x):\n",
        "    if self.softmax: return tf.nn.softmax(self.model(x))\n",
        "    return self.model(x)\n",
        "\n",
        "# def EncapsulationModel(sub_model, output_units, softmax=True):\n",
        "#   model = tf.keras.Sequential()\n",
        "#   model.add(sub_model)\n",
        "#   for _ in range(2):\n",
        "#     model.add(tf.keras.layers.LSTM(output_units, \n",
        "#                                    return_sequences=True))\n",
        "#   if softmax: model.add(tf.keras.layers.Softmax())\n",
        "\n",
        "#   return model\n",
        "    \n",
        "class RNNT(tf.keras.Model):\n",
        "  def __init__(self, audio_model, text_model, output_units):\n",
        "    super(RNNT, self).__init__()\n",
        "    \n",
        "    self.audio_model = tf.keras.Sequential()\n",
        "    self.audio_model.add(audio_model)\n",
        "    for i in range(3):\n",
        "      self.audio_model.add(tf.keras.layers.SeparableConv1D(1024, 2, 2))\n",
        "    self.audio_model.add(tf.keras.layers.Dense(512))\n",
        "      \n",
        "    self.text_model = tf.keras.Sequential()\n",
        "    self.text_model.add(text_model)\n",
        "    self.text_model.add(tf.keras.layers.Dense(512))\n",
        "    \n",
        "    self.joint_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(512),\n",
        "        tf.keras.layers.Dense(output_units)\n",
        "    ])\n",
        "    \n",
        "  @tf.function\n",
        "  def joint(self, x_audio, x_text):\n",
        "    broadcast_shape = tf.broadcast_dynamic_shape(tf.shape(x_audio)[:-1], tf.shape(x_text)[:-1])\n",
        "    x_audio = tf.broadcast_to(x_audio, tf.concat([broadcast_shape, tf.shape(x_audio)[-1:]], axis=0))\n",
        "    x_text = tf.broadcast_to(x_text, tf.concat([broadcast_shape, tf.shape(x_text)[-1:]], axis=0))\n",
        "    x = tf.concat([x_audio, x_text], axis=-1) # Batch, Time-Steps, Graphemes, Channels (Audio_Channels + Text_Channels)\n",
        "    return tf.nn.log_softmax(self.joint_model(x))\n",
        "    \n",
        "  @tf.function\n",
        "  def call(self, x):\n",
        "    x_audio, x_text = x\n",
        "    x_audio = self.audio_model(x_audio) # Batch, Time-Steps, Channels\n",
        "    x_text = self.text_model(x_text) # Batch, Graphemes, Channels\n",
        "    x_audio = tf.expand_dims(x_audio, axis=2) # Batch, Time-Steps, 1, Channels\n",
        "    x_text = tf.expand_dims(x_text, axis=1) # Batch 1, Graphemes, Channels\n",
        "    return self.joint(x_audio, x_text)\n",
        "  \n",
        "  def beam_search(self, audio, width=10, prefix=True, debug=False):\n",
        "    def forward_step(labels, hidden=None):\n",
        "      # All labels\n",
        "      labels = tf.reshape(tf.one_hot([labels], label_size), (1, 1, label_size))\n",
        "      return self.text_model(labels)[:, -1, :], hidden\n",
        "\n",
        "    def is_prefix(pref, seq):\n",
        "      if pref == seq or len(pref) >= len(seq): return False\n",
        "      for i in range(len(pref)):\n",
        "        if pref[i] != seq[i]: return False\n",
        "      return True\n",
        "    \n",
        "    def log_aplusb(a, b):\n",
        "      return max(a, b) + math.log1p(math.exp(-math.fabs(a-b)))\n",
        "\n",
        "    audio = self.audio_model(audio) # Batch, Time-Steps, Channels\n",
        "    B = [Sequence()]\n",
        "    if debug: pbar = tqdm.tqdm(total = audio.shape[1], position=0)\n",
        "    if debug: print()\n",
        "    for i in range(audio.shape[1]):\n",
        "      x = audio[:, i, :]\n",
        "      if prefix: sorted(B, key=lambda a: len(a.k), reverse=True) # larger sequence first add\n",
        "      if debug: pbar.update()\n",
        "      if debug: pbar.set_postfix_str(s = B[0])\n",
        "      A = B\n",
        "      B = []\n",
        "      first_print = False\n",
        "      if prefix:\n",
        "        for j in range(len(A)-1):\n",
        "          for i in range(j+1, len(A)):\n",
        "            if not is_prefix(A[i].k, A[j].k): continue\n",
        "            if debug:\n",
        "              if first_print: print()\n",
        "              print(\"  Prefix: {} -> {}\".format(A[i].string(), A[j].string()))\n",
        "            # A[i] -> A[j]\n",
        "            pred, _ = forward_step(A[i].k[-1], A[i].h)\n",
        "            idx = len(A[i].k)\n",
        "            logp = self.joint(x, pred)[0, :]\n",
        "            curlogp = A[i].logp + float(logp[A[j].k[idx]])\n",
        "            for k in range(idx, len(A[j].k)-1):\n",
        "              logp = self.joint(x, A[j].g[k])[0, :]\n",
        "              curlogp += float(logp[A[j].k[k+1]])\n",
        "            A[j].logp = log_aplusb(A[j].logp, curlogp)\n",
        "            \n",
        "      for i in A:\n",
        "        i.resetOrig()\n",
        "      \n",
        "      while len(A) > 0:\n",
        "        # y* = most probable in A\n",
        "        y_hat = max(A, key=lambda a: a.logp)\n",
        "        # remove y* from A\n",
        "        A.remove(y_hat)\n",
        "        # calculate P(k|y_hat, t)\n",
        "        # get last label and hidden state\n",
        "        pred, hidden = forward_step(y_hat.k[-1], y_hat.h)\n",
        "        logp = self.joint(x, pred)[0, :]\n",
        "        # for k \\in vocab\n",
        "        for k in range(label_size):\n",
        "          yk = Sequence(y_hat)\n",
        "          yk.logp += float(logp[k])\n",
        "          if k == 0:\n",
        "              B.append(yk) # next move\n",
        "              if len(yk.g) - len(yk.orig) > 60:\n",
        "                print(yk.g, yk.orig)\n",
        "                break\n",
        "              continue\n",
        "          # store prediction distribution and last hidden state\n",
        "          yk.h = hidden; yk.k.append(k); \n",
        "          if prefix: yk.g.append(pred)\n",
        "          A.append(yk)\n",
        "        y_hat = max(A, key=lambda a: a.logp)\n",
        "        yb = max(B, key=lambda b: b.logp)\n",
        "        if len(B) >= width and yb.logp >= y_hat.logp: break\n",
        "\n",
        "      # beam width\n",
        "      sorted(B, key=lambda b: b.logp, reverse=True)\n",
        "      B = B[:width]\n",
        "\n",
        "    if debug: print()\n",
        "    # return highest probability sequence\n",
        "    return B[0].string(), -B[0].logp\n",
        "  \n",
        "\n",
        "class SimpleLSTM(tf.keras.Model):\n",
        "  def __init__(self, output_units):\n",
        "    super(SimpleLSTM, self).__init__()\n",
        "    self.model = [\n",
        "        tf.keras.layers.Dense(256)\n",
        "    ] + [\n",
        "        tf.keras.layers.LSTM(1024,\n",
        "                                 return_sequences=True, recurrent_activation='hard_sigmoid') for _ in range(7)\n",
        "    ] + [\n",
        "        tf.keras.layers.Dense(output_units)\n",
        "    ]\n",
        "    self.model = tf.keras.Sequential(self.model)\n",
        "    \n",
        "  @tf.function\n",
        "  def call(self, x):\n",
        "    return tf.nn.softmax(self.model(x))\n",
        "\n",
        "def model_test():\n",
        "  pass\n",
        "# model = RNNT(label_size)\n",
        "\n",
        "# out = model(next(iter(dataset))[0])\n",
        "\n",
        "# for i in out[1][0].numpy():\n",
        "#   print(i)\n",
        "\n",
        "# x, y, x_length, y_length = next(iter(dataset))\n",
        "# y = tf.one_hot(y, label_size)\n",
        "# print(x.shape, y.shape, x_length.shape, y_length.shape)\n",
        "\n",
        "# print(x.shape, y.shape)\n",
        "# with tf.GradientTape() as tape:\n",
        "#   output = model(x)\n",
        "#   print(output)\n",
        "\n",
        "# print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBKD9ui4jirB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_and_stack(data):\n",
        "  max_data_length = max(map(len, data))\n",
        "  padded_data = [np.pad(datum, (0, max_data_length - len(datum)), mode='constant', constant_values=0) for datum in data]\n",
        "  data = np.stack(padded_data, axis=0)\n",
        "  \n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx6M8etHkgnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "  iterator = iter(dataset)\n",
        "  for data, targets in iterator:\n",
        "    data, targets = next(iterator)\n",
        "    pred = model(data)\n",
        "    print(tf.reduce_mean(loss(targets, pred)))\n",
        "  return loss(targets, pred)\n",
        "\n",
        "# test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cyhTE5-g6j2b",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def y_processing(y):\n",
        "  y = tf.cast(y, tf.int32)\n",
        "  audio_length, text_length, text = y[:, 0, 0], y[:, 1, 0], y[:, 2:, 0]\n",
        "  return audio_length, text_length, text\n",
        "\n",
        "@tf.function\n",
        "def ctc_loss(y, x):\n",
        "  audio_length, text_length, text = y_processing(y)\n",
        "  loss_value = tf.nn.ctc_loss(text, x, text_length, audio_length, logits_time_major = False)\n",
        "  loss_value = tf.reduce_mean(loss_value)\n",
        "  return loss_value\n",
        "\n",
        "@tf.function\n",
        "def loss(y, x):\n",
        "  audio_length, text_length, text = y_processing(y)\n",
        "  loss_value = warprnnt_tensorflow.rnnt_loss(x, text, tf.cast(tf.math.ceil(audio_length / 2**3), tf.int32), text_length)\n",
        "  loss_value = tf.reduce_mean(loss_value)\n",
        "  return loss_value\n",
        "# #   label_data = pad_and_stack(targets)\n",
        "# #   label_data = tf.keras.backend.ctc_label_dense_to_sparse(targets, [len(i) for i in targets])\n",
        "# #   loss_value = tf.reduce_mean(inputs)\n",
        "#   loss_value = tf.nn.ctc_loss(label_data, pred, label_length=label_length, logit_length=logits_length, logits_time_major=False)\n",
        "# #   loss_value = tf.keras.backend.ctc_batch_cost(label_data, pred, tf.constant([pred.shape[2] for _ in range(pred.shape[0])], shape=(pred.shape[0], 1)), tf.constant([len(i) for i in targets], shape=(pred.shape[0], 1)))\n",
        "#   loss_value = tf.reduce_mean(loss_value)\n",
        "#   return loss_value\n",
        "\n",
        "EPOCHS = 30\n",
        "\n",
        "# Text Training\n",
        "text_model = TextModel()\n",
        "encap_model = EncapsulationModel(text_model, label_size)\n",
        "\n",
        "encap_model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['categorical_accuracy'])\n",
        "encap_model.fit(text_dataset.take(10), epochs=1)\n",
        "\n",
        "# Audio_Training\n",
        "audio_model = AudioModel()\n",
        "encap_model = EncapsulationModel(audio_model, label_size, False)\n",
        "\n",
        "test_x, test_y = next(iter(audio_dataset))\n",
        "\n",
        "encap_model.compile(loss=ctc_loss,\n",
        "                    optimizer='nadam')\n",
        "encap_model.fit(audio_dataset.take(10), epochs=1)\n",
        "\n",
        "encap_model = None\n",
        "\n",
        "!mkdir tmp\n",
        "\n",
        "text_model.save_weights(\"tmp/text_model.h5\", overwrite=True)\n",
        "\n",
        "audio_model.save_weights(\"tmp/audio_model.h5\", overwrite=True)\n",
        "\n",
        "# Regular RNNT Training\n",
        "\n",
        "model = RNNT(audio_model, text_model, label_size)\n",
        "\n",
        "def test():\n",
        "  data = next(iter(dataset))\n",
        "  loss(data[1], model(data[0]))\n",
        "  \n",
        "# test()\n",
        "\n",
        "checkpoint_path = \"training/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    verbose=1, \n",
        "    save_weights_only=True,\n",
        "    save_freq=100)\n",
        "\n",
        "import multiprocessing as mp\n",
        "\n",
        "class TestCallback(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, model, dataset, steps=100):\n",
        "    self.model = model\n",
        "    self.dataset = iter(dataset.shuffle(10))\n",
        "    self.steps = steps\n",
        "  \n",
        "  def on_train_batch_begin(self, batch, logs=None):\n",
        "    if not batch % self.steps and batch:\n",
        "      datum = next(self.dataset)\n",
        "      print()\n",
        "      print(\" Predicted: {}\\n Real: {}\".format(self.model.beam_search(datum[0][0], width=10, debug=True), ''.join([labels[i] for i in datum[1][0, 2:, 0, 0]])))\n",
        "    \n",
        "test_callback = TestCallback(\n",
        "    model = model,\n",
        "    dataset = test_dataset\n",
        ")\n",
        "      \n",
        "\n",
        "model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "# model.load_weights(latest)\n",
        "\n",
        "model.compile(loss = loss,\n",
        "              optimizer = 'nadam')\n",
        "\n",
        "model.fit(dataset.take(1))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(dataset, \n",
        "          callbacks=[cp_callback, test_callback], \n",
        "          epochs=EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSgDCLJmhYB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## No layer norm LSTM (2048) -> Dense (512)\n",
        "#    8/5736 [..............................] - ETA: 55:26:05 - loss: 757.5204\n",
        "#   5406/Unknown - 15339s 3s/step - loss: 845.4339\n",
        "\n",
        "# Epoch 1/30\n",
        "# 5736/5736 [==============================] - 16274s 3s/step - loss: 844.9601\n",
        "# Epoch 2/30\n",
        "#   50/5736 [..............................] - ETA: 4:00:08 - loss: 777.5321\n",
        "\n",
        "## LSTM (2048) -> layer norm -> Dense (512)\n",
        "\n",
        "# Trainable params: 214,770,246\n",
        "# Non-trainable params: 0\n",
        "# _________________________________________________________________\n",
        "# Epoch 1/30\n",
        "#      99/Unknown - 284s 3s/step - loss: 1062.7551\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#     199/Unknown - 578s 3s/step - loss: 1035.7189\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#     299/Unknown - 866s 3s/step - loss: 1014.6336\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#     399/Unknown - 1156s 3s/step - loss: 1006.8043\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#     476/Unknown - 1386s 3s/step - loss: 1009.3790\n",
        "\n",
        "## LSTM (1024) -> layer norm\n",
        "\n",
        "# Trainable params: 77,385,286\n",
        "# Non-trainable params: 0\n",
        "# _________________________________________________________________\n",
        "# Epoch 1/30\n",
        "#      99/Unknown - 198s 2s/step - loss: 840.7825\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#     199/Unknown - 400s 2s/step - loss: 837.4487\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#     299/Unknown - 604s 2s/step - loss: 844.7735\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#     399/Unknown - 810s 2s/step - loss: 852.7807\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#     499/Unknown - 1012s 2s/step - loss: 844.3974\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#     599/Unknown - 1217s 2s/step - loss: 845.8188\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#     699/Unknown - 1419s 2s/step - loss: 842.7905\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#     799/Unknown - 1617s 2s/step - loss: 831.0107\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#     899/Unknown - 1818s 2s/step - loss: 828.8739\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#     999/Unknown - 2026s 2s/step - loss: 834.0196\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#    1099/Unknown - 2229s 2s/step - loss: 830.7841\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#    1199/Unknown - 2429s 2s/step - loss: 827.2642\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#    1299/Unknown - 2629s 2s/step - loss: 824.3682\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#    1399/Unknown - 2829s 2s/step - loss: 820.7671\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#    1499/Unknown - 3035s 2s/step - loss: 823.5107\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#    1599/Unknown - 3232s 2s/step - loss: 818.1643\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#    1699/Unknown - 3427s 2s/step - loss: 810.9173\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#    1799/Unknown - 3634s 2s/step - loss: 814.3912\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#    1899/Unknown - 3838s 2s/step - loss: 815.0992\n",
        "# Epoch 00001: saving model to training/cp-0001.ckpt\n",
        "#    1924/Unknown - 3894s 2s/step - loss: 817.5160\n",
        "\n",
        "## LSTM(512) -> concat"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}