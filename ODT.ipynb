{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ODT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VVfIT5qvDxL8",
        "outputId": "9fa8db8a-0cbe-455b-c195-87859e2c6162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "try:\n",
        "  # Notebook specific code\n",
        "\n",
        "  %matplotlib inline\n",
        "  %tensorflow_version 2.x \n",
        "  # !pip install tensorflow-gpu==2.0.0-beta1 -q\n",
        "  \n",
        "  !pip install gast==0.2.2 --force-reinstall\n",
        "  \n",
        "  try:\n",
        "    import tensorflow as tf\n",
        "    import warprnnt_tensorflow\n",
        "  except:\n",
        "    !git clone https://github.com/HawkAaron/warp-transducer\n",
        "    !export CUDA_HOME=/usr/local/cuda; cd warp-transducer; mkdir build; cd build; cmake -DCUDA_TOOLKIT_ROOT_DIR=$CUDA_HOME ..; make\n",
        "    !cd warp-transducer/tensorflow_binding; pip install -e .\n",
        "    \n",
        "    import os\n",
        "    os.kill(os.getpid(), 9)\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Collecting gast==0.2.2\n",
            "Installing collected packages: gast\n",
            "  Found existing installation: gast 0.2.2\n",
            "    Uninstalling gast-0.2.2:\n",
            "      Successfully uninstalled gast-0.2.2\n",
            "Successfully installed gast-0.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgI8WSyIfjtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import warprnnt_tensorflow\n",
        "\n",
        "def gpu_config(mem_limit=None, print_gpus=True, log_device_placement=False):\n",
        "  tf.debugging.set_log_device_placement(log_device_placement)\n",
        "\n",
        "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "  if print_gpus:\n",
        "    print(gpus)\n",
        "  if mem_limit is not None and gpus:\n",
        "    try:\n",
        "      tf.config.experimental.set_virtual_device_configuration(\n",
        "            gpus[0],\n",
        "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*10)])\n",
        "    except RuntimeError as e:\n",
        "      # Virtual devices must be set before GPUs have been initialized\n",
        "      print(e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36MJnQH4JQfY",
        "colab_type": "text"
      },
      "source": [
        "This is the code for the On Device Transcription Model, based of [this blog post](https://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7Gq6Gk0PQf8",
        "colab_type": "text"
      },
      "source": [
        "Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWOzMUKuADMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import urllib.request\n",
        "import shutil\n",
        "\n",
        "import tarfile\n",
        "\n",
        "def download_data():\n",
        "  url = \"http://www.openslr.org/resources/60/dev-clean.tar.gz\"\n",
        "  file_name = \"dev-clean.tar.gz\"\n",
        "  dir_name = \"LibriTTS/dev-clean\"\n",
        "\n",
        "  if not os.path.isdir(dir_name):\n",
        "    with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
        "      shutil.copyfileobj(response, out_file)\n",
        "    tar = tarfile.open(file_name)\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "    \n",
        "    os.remove(file_name)\n",
        "    \n",
        "  files = []\n",
        "  for (dirpath, dirnames, filenames) in os.walk(dir_name):\n",
        "      for filename in filenames:\n",
        "        filepath = dirpath + \"/\" + filename\n",
        "        if filepath.endswith(\".wav\"):\n",
        "          files.append(filepath[:-4])\n",
        "  \n",
        "  return files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL2oITPZXB9c",
        "colab_type": "text"
      },
      "source": [
        "Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag-7WqgucSTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "labels = [''] + [chr(i) for i in range(ord('a'), ord('z') + 1)] + [' '] + list(string.digits + string.punctuation)\n",
        "label_lookup = {}\n",
        "for i in range(len(labels)):\n",
        "  label_lookup[labels[i]] = i\n",
        "label_size = len(labels)\n",
        "\n",
        "def text_normalise(text_data):\n",
        "  labels_data = [label_lookup[x] for x in text_data.lower()]\n",
        "\n",
        "  return labels_data\n",
        "\n",
        "import librosa, librosa.display\n",
        "import numpy as np\n",
        "\n",
        "def get_data(filepath, sr=24000):\n",
        "  filepath = bytes.decode(filepath)\n",
        "  y, _ = librosa.load(filepath + \".wav\", mono=True, sr=sr)\n",
        "  \n",
        "  y = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=int(25*sr/1000), hop_length=int(10*sr/1000), n_mels=80)\n",
        "  y = librosa.power_to_db(y, ref=np.max)\n",
        "  \n",
        "  with open(filepath + \".original.txt\", 'r') as f:\n",
        "    text = f.read()\n",
        "  return y.astype(np.float32), text_normalise(text), len(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEaj4JLUOMCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset(files, audio_batches, text_batches, rnnt_batches, caching=False):\n",
        "  @tf.function\n",
        "  def data_shapping(audio, text, text_length):\n",
        "    text = tf.cast(text, tf.int32)\n",
        "    text_length = tf.cast(text_length, tf.int32)\n",
        "\n",
        "    audio_shape = tf.shape(audio)\n",
        "    audio = tf.pad(audio, [[0, 0], [0, 3 - tf.math.floormod(audio_shape[1], 3)]], constant_values = -80.0)\n",
        "    audio_shape = tf.shape(audio)\n",
        "    audio = tf.reshape(tf.transpose(audio), (audio_shape[1]//3, 3 * audio_shape[0]))\n",
        "    return audio, tf.one_hot(tf.concat([[0], text], axis=-1), label_size, dtype=tf.float32), tf.reshape(tf.concat([[tf.cast(tf.shape(audio)[0], tf.int32)], [text_length], text], axis=-1), (-1, 1))\n",
        "    #(audio, text_embedded, [audio_length, text_length, text])\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(files)\n",
        "  dataset = dataset.shuffle(buffer_size=10000)\n",
        "\n",
        "  # dataset to be both\n",
        "  dataset = dataset.map(lambda x: tf.numpy_function(get_data, [x], [tf.float32, tf.int64, tf.int64]))\n",
        "  dataset = dataset.map(data_shapping, 16)\n",
        "  if caching: dataset.cache()\n",
        "\n",
        "  @tf.function\n",
        "  def audio_crop(audio, text_embedded, text_and_length):\n",
        "    return audio, text_and_length\n",
        "\n",
        "  # Split audio\n",
        "  audio_dataset = dataset.map(audio_crop, 16)\n",
        "\n",
        "  @tf.function\n",
        "  def text_crop(audio, text_embedded, text_and_length):\n",
        "    return text_embedded[:-1], text_embedded[1:]\n",
        "\n",
        "  # Split text\n",
        "  text_dataset = dataset.map(text_crop, 16)\n",
        "\n",
        "  # Test dataset split\n",
        "\n",
        "  test_dataset = dataset.take(1000)\n",
        "\n",
        "  # Batching split datasets\n",
        "\n",
        "  text_dataset = text_dataset.padded_batch(text_batches, padded_shapes=((512, label_size), (512, label_size))).prefetch(20)\n",
        "  audio_dataset = audio_dataset.padded_batch(audio_batches, padded_shapes=((1024, 80*3), (512, 1)), drop_remainder=True).prefetch(20)\n",
        "\n",
        "  # Batching RNNT train & test datasets\n",
        "\n",
        "  @tf.function\n",
        "  def data_grouping(audio, text_embedded, targets):\n",
        "    return ((audio, text_embedded), targets)\n",
        "\n",
        "  def dataset_shapping(dataset, batches=1):\n",
        "  # dataset = dataset.padded_batch(1, padded_shapes=((None, 80*3), (None, ), (), ())).prefetch(2) # 10 loops, best of 3: 393 ms per loop\n",
        "    dataset = dataset.padded_batch(batches, padded_shapes=((1024, 80*3), (512, label_size), (512, 1))).prefetch(20) # 10 loops, best of 3: 461 ms per loop\n",
        "\n",
        "    dataset = dataset.map(data_grouping, 2)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "  dataset = dataset_shapping(dataset, rnnt_batches)\n",
        "  test_dataset = dataset_shapping(test_dataset, 1).repeat()\n",
        "  \n",
        "  return audio_dataset, text_dataset, dataset, test_dataset\n",
        "\n",
        "# next(iter(text_dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufnq3elmz1mc",
        "colab_type": "text"
      },
      "source": [
        "Max frame size: 1296"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOzQlDvq2qJI",
        "colab_type": "text"
      },
      "source": [
        "Max text length: 419"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWhwGOunTZIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "class Sequence():\n",
        "  def __init__(self, seq=None, hidden=None, blank=0):\n",
        "    if seq is None:\n",
        "      self.g = [] # predictions of phoneme language model\n",
        "      self.k = [blank] # prediction phoneme label\n",
        "      self.orig = []\n",
        "      # self.h = [None] # input hidden vector to phoneme model\n",
        "      self.h = hidden\n",
        "      self.logp = 0 # probability of this sequence, in log scale\n",
        "    else:\n",
        "      self.g = seq.g[:] # save for prefixsum\n",
        "      self.k = seq.k[:]\n",
        "      self.orig = seq.orig[:]\n",
        "      self.h = seq.h\n",
        "      self.logp = seq.logp\n",
        "  \n",
        "  def resetOrig(self):\n",
        "    self.orig = self.k\n",
        "\n",
        "  def __str__(self):\n",
        "    return 'Prediction: {} ({:.2f})'.format(self.string(), -self.logp)\n",
        "  \n",
        "  def string(self):\n",
        "    return ''.join([labels[i] for i in self.k])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj1XR1bwH9fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tqdm\n",
        "\n",
        "def AudioModel():\n",
        "  audio_model = tf.keras.Sequential()\n",
        "  audio_model.add(tf.keras.layers.Masking(mask_value=0.))\n",
        "  for i in range(8):\n",
        "    audio_model.add(tf.keras.layers.LSTM(1024,#2048,\n",
        "                                         return_sequences=True))#, recurrent_activation='hard_sigmoid'))\n",
        "    audio_model.add(tf.keras.layers.LayerNormalization(2))\n",
        "  \n",
        "  return audio_model\n",
        "\n",
        "def TextModel():\n",
        "  text_model = tf.keras.Sequential()\n",
        "  text_model.add(tf.keras.layers.Masking(mask_value=0.))\n",
        "  for _ in range(5):\n",
        "    text_model.add(tf.keras.layers.LSTM(1024,\n",
        "                                        return_sequences=True))#, recurrent_activation='hard_sigmoid'))\n",
        "    text_model.add(tf.keras.layers.LayerNormalization(2))\n",
        "    \n",
        "  return text_model\n",
        "\n",
        "class EncapsulationModel(tf.keras.Model):\n",
        "  def __init__(self, sub_model, output_units, softmax=True):\n",
        "    super(EncapsulationModel, self).__init__()\n",
        "    \n",
        "    self.model = tf.keras.Sequential()\n",
        "    self.model.add(sub_model)\n",
        "    for _ in range(2):\n",
        "      self.model.add(tf.keras.layers.LSTM(output_units,\n",
        "                                         return_sequences=True))\n",
        "    self.softmax = softmax\n",
        "    \n",
        "  @tf.function\n",
        "  def call(self, x):\n",
        "    if self.softmax: return tf.nn.softmax(self.model(x))\n",
        "    return self.model(x)\n",
        "\n",
        "# def EncapsulationModel(sub_model, output_units, softmax=True):\n",
        "#   model = tf.keras.Sequential()\n",
        "#   model.add(sub_model)\n",
        "#   for _ in range(2):\n",
        "#     model.add(tf.keras.layers.LSTM(output_units, \n",
        "#                                    return_sequences=True))\n",
        "#   if softmax: model.add(tf.keras.layers.Softmax())\n",
        "\n",
        "#   return model\n",
        "\n",
        "class JointModel(tf.keras.Model):\n",
        "  def __init__(self, output_units):\n",
        "    super(JointModel, self).__init__()\n",
        "    \n",
        "    self.post_audio = tf.keras.Sequential()\n",
        "    for i in range(3):\n",
        "      self.post_audio.add(tf.keras.layers.SeparableConv1D(1024, 2, 2))\n",
        "    self.post_audio.add(tf.keras.layers.Dense(512))\n",
        "    \n",
        "    self.post_text = tf.keras.Sequential()\n",
        "    self.post_text.add(tf.keras.layers.Dense(512))\n",
        "    \n",
        "    self.joint_model = tf.keras.Sequential([\n",
        "          tf.keras.layers.Dense(512),\n",
        "          tf.keras.layers.Dense(output_units)\n",
        "    ])\n",
        "    \n",
        "  @tf.function\n",
        "  def call(self, x_audio, x_text):\n",
        "    x_audio = self.post_audio(x_audio)\n",
        "    x_text = self.post_text(x_text)\n",
        "    \n",
        "    x_audio = tf.expand_dims(x_audio, axis=2) # Batch, Time-Steps, 1, Channels\n",
        "    x_text = tf.expand_dims(x_text, axis=1) # Batch 1, Graphemes, Channels\n",
        "    broadcast_shape = tf.broadcast_dynamic_shape(tf.shape(x_audio)[:-1], tf.shape(x_text)[:-1])\n",
        "    x_audio = tf.broadcast_to(x_audio, tf.concat([broadcast_shape, tf.shape(x_audio)[-1:]], axis=0))\n",
        "    x_text = tf.broadcast_to(x_text, tf.concat([broadcast_shape, tf.shape(x_text)[-1:]], axis=0))\n",
        "    x = tf.concat([x_audio, x_text], axis=-1) # Batch, Time-Steps, Graphemes, Channels (Audio_Channels + Text_Channels)\n",
        "    return tf.nn.log_softmax(self.joint_model(x))\n",
        "    \n",
        "class RNNT(tf.keras.Model):\n",
        "  def __init__(self, audio_model, text_model, joint_model):\n",
        "    super(RNNT, self).__init__()\n",
        "    \n",
        "    self.audio_model = audio_model\n",
        "    self.text_model = text_model\n",
        "    self.joint = joint_model\n",
        "    \n",
        "  @tf.function\n",
        "  def call(self, x):\n",
        "    x_audio, x_text = x\n",
        "    x_audio = self.audio_model(x_audio) # Batch, Time-Steps, Channels\n",
        "    x_text = self.text_model(x_text) # Batch, Graphemes, Channels\n",
        "    return self.joint(x_audio, x_text)\n",
        "  \n",
        "  def beam_search(self, audio, width=10, prefix=True, debug=False):\n",
        "    def forward_step(labels, hidden=None):\n",
        "      # All labels\n",
        "      labels = tf.reshape(tf.one_hot([labels], label_size), (1, 1, label_size))\n",
        "      return self.text_model(labels)[:, -1, :], hidden\n",
        "\n",
        "    def is_prefix(pref, seq):\n",
        "      if pref == seq or len(pref) >= len(seq): return False\n",
        "      for i in range(len(pref)):\n",
        "        if pref[i] != seq[i]: return False\n",
        "      return True\n",
        "    \n",
        "    def log_aplusb(a, b):\n",
        "      return max(a, b) + math.log1p(math.exp(-math.fabs(a-b)))\n",
        "\n",
        "    audio = self.audio_model(audio) # Batch, Time-Steps, Channels\n",
        "    B = [Sequence()]\n",
        "    if debug: pbar = tqdm.tqdm(total = audio.shape[1], position=0)\n",
        "    if debug: print()\n",
        "    for i in range(audio.shape[1]):\n",
        "      x = audio[:, i, :]\n",
        "      if prefix: sorted(B, key=lambda a: len(a.k), reverse=True) # larger sequence first add\n",
        "      if debug: pbar.update()\n",
        "      if debug: pbar.set_postfix_str(s = B[0])\n",
        "      A = B\n",
        "      B = []\n",
        "      first_print = False\n",
        "      if prefix:\n",
        "        for j in range(len(A)-1):\n",
        "          for i in range(j+1, len(A)):\n",
        "            if not is_prefix(A[i].k, A[j].k): continue\n",
        "            if debug:\n",
        "              if first_print: print()\n",
        "              print(\"  Prefix: {} -> {}\".format(A[i].string(), A[j].string()))\n",
        "            # A[i] -> A[j]\n",
        "            pred, _ = forward_step(A[i].k[-1], A[i].h)\n",
        "            idx = len(A[i].k)\n",
        "            logp = self.joint(x, pred)[0, :]\n",
        "            curlogp = A[i].logp + float(logp[A[j].k[idx]])\n",
        "            for k in range(idx, len(A[j].k)-1):\n",
        "              logp = self.joint(x, A[j].g[k])[0, :]\n",
        "              curlogp += float(logp[A[j].k[k+1]])\n",
        "            A[j].logp = log_aplusb(A[j].logp, curlogp)\n",
        "            \n",
        "      for i in A:\n",
        "        i.resetOrig()\n",
        "      \n",
        "      while len(A) > 0:\n",
        "        # y* = most probable in A\n",
        "        y_hat = max(A, key=lambda a: a.logp)\n",
        "        # remove y* from A\n",
        "        A.remove(y_hat)\n",
        "        # calculate P(k|y_hat, t)\n",
        "        # get last label and hidden state\n",
        "        pred, hidden = forward_step(y_hat.k[-1], y_hat.h)\n",
        "        logp = self.joint(x, pred)[0, :]\n",
        "        # for k \\in vocab\n",
        "        for k in range(label_size):\n",
        "          yk = Sequence(y_hat)\n",
        "          yk.logp += float(logp[k])\n",
        "          if k == 0:\n",
        "              B.append(yk) # next move\n",
        "              if len(yk.g) - len(yk.orig) > 60:\n",
        "                print(yk.g, yk.orig)\n",
        "                break\n",
        "              continue\n",
        "          # store prediction distribution and last hidden state\n",
        "          yk.h = hidden; yk.k.append(k); \n",
        "          if prefix: yk.g.append(pred)\n",
        "          A.append(yk)\n",
        "        y_hat = max(A, key=lambda a: a.logp)\n",
        "        yb = max(B, key=lambda b: b.logp)\n",
        "        if len(B) >= width and yb.logp >= y_hat.logp: break\n",
        "\n",
        "      # beam width\n",
        "      sorted(B, key=lambda b: b.logp, reverse=True)\n",
        "      B = B[:width]\n",
        "\n",
        "    if debug: print()\n",
        "    # return highest probability sequence\n",
        "    return B[0].string(), -B[0].logp\n",
        "  \n",
        "\n",
        "class SimpleLSTM(tf.keras.Model):\n",
        "  def __init__(self, output_units):\n",
        "    super(SimpleLSTM, self).__init__()\n",
        "    self.model = [\n",
        "        tf.keras.layers.Dense(256)\n",
        "    ] + [\n",
        "        tf.keras.layers.LSTM(1024,\n",
        "                                 return_sequences=True, recurrent_activation='hard_sigmoid') for _ in range(7)\n",
        "    ] + [\n",
        "        tf.keras.layers.Dense(output_units)\n",
        "    ]\n",
        "    self.model = tf.keras.Sequential(self.model)\n",
        "    \n",
        "  @tf.function\n",
        "  def call(self, x):\n",
        "    return tf.nn.softmax(self.model(x))\n",
        "\n",
        "def model_test(): #TODO: Implement model tests\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBKD9ui4jirB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_and_stack(data):\n",
        "  max_data_length = max(map(len, data))\n",
        "  padded_data = [np.pad(datum, (0, max_data_length - len(datum)), mode='constant', constant_values=0) for datum in data]\n",
        "  data = np.stack(padded_data, axis=0)\n",
        "  \n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx6M8etHkgnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "  iterator = iter(dataset)\n",
        "  for data, targets in iterator:\n",
        "    data, targets = next(iterator)\n",
        "    pred = model(data)\n",
        "    print(tf.reduce_mean(loss(targets, pred)))\n",
        "  return loss(targets, pred)\n",
        "\n",
        "# test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cyhTE5-g6j2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "8ee0d3b0-d879-45ad-924d-0fa72edb6892"
      },
      "source": [
        "@tf.function\n",
        "def y_processing(y):\n",
        "  y = tf.cast(y, tf.int32)\n",
        "  audio_length, text_length, text = y[:, 0, 0], y[:, 1, 0], y[:, 2:, 0]\n",
        "  return audio_length, text_length, text\n",
        "\n",
        "@tf.function\n",
        "def ctc_loss(y, x):\n",
        "  audio_length, text_length, text = y_processing(y)\n",
        "  loss_value = tf.nn.ctc_loss(text, x, text_length, audio_length, logits_time_major = False)\n",
        "  loss_value = tf.reduce_mean(loss_value)\n",
        "  return loss_value\n",
        "\n",
        "@tf.function\n",
        "def loss(y, x):\n",
        "  audio_length, text_length, text = y_processing(y)\n",
        "  loss_value = warprnnt_tensorflow.rnnt_loss(x, text, tf.cast(tf.math.ceil(audio_length / 2**3), tf.int32), text_length)\n",
        "  loss_value = tf.reduce_mean(loss_value)\n",
        "  return loss_value\n",
        "# #   label_data = pad_and_stack(targets)\n",
        "# #   label_data = tf.keras.backend.ctc_label_dense_to_sparse(targets, [len(i) for i in targets])\n",
        "# #   loss_value = tf.reduce_mean(inputs)\n",
        "#   loss_value = tf.nn.ctc_loss(label_data, pred, label_length=label_length, logit_length=logits_length, logits_time_major=False)\n",
        "# #   loss_value = tf.keras.backend.ctc_batch_cost(label_data, pred, tf.constant([pred.shape[2] for _ in range(pred.shape[0])], shape=(pred.shape[0], 1)), tf.constant([len(i) for i in targets], shape=(pred.shape[0], 1)))\n",
        "#   loss_value = tf.reduce_mean(loss_value)\n",
        "#   return loss_value\n",
        "\n",
        "class TestCallback(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, model, dataset, steps=100):\n",
        "    self.model = model\n",
        "    self.dataset = iter(dataset.shuffle(10))\n",
        "    self.steps = steps\n",
        "\n",
        "  def on_train_batch_begin(self, batch, logs=None):\n",
        "    if not batch % self.steps and batch:\n",
        "      datum = next(self.dataset)\n",
        "      print()\n",
        "      print(\" Predicted: {}\\n Real: {}\".format(self.model.beam_search(datum[0][0], width=10, debug=True), ''.join([labels[i] for i in datum[1][0, 2:, 0, 0]])))\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[33m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "    BLINK = '\\033[5m'\n",
        "  \n",
        "import shutil\n",
        "\n",
        "audio_batches = 8\n",
        "text_batches = 64\n",
        "rnnt_batches= 4\n",
        "\n",
        "def main():\n",
        "  \n",
        "  gpu_config(1024*12, print_gpus=True, log_device_placement=False)  \n",
        "  \n",
        "  files = download_data()\n",
        "  audio_dataset, text_dataset, dataset, test_dataset = get_dataset(files, audio_batches=audio_batches, text_batches=text_batches, rnnt_batches=rnnt_batches, caching=True)\n",
        "  \n",
        "  EPOCHS = 30\n",
        "  \n",
        "  # Initialising models\n",
        "  text_model = TextModel()\n",
        "  audio_model = AudioModel()\n",
        "  joint_model = JointModel(label_size)\n",
        "  \n",
        "  checkpoint_dir = \"checkpoints\"\n",
        "  current_dir = checkpoint_dir + \"/current\"\n",
        "  text_checkpoint_path = current_dir + \"/cp-text-{batch:04d}.ckpt\"\n",
        "  audio_checkpoint_path = current_dir + \"/cp-audio-{batch:04d}.ckpt\"\n",
        "  joint_checkpoint_path = current_dir + \"/cp-joint-{batch:04d}.ckpt\"\n",
        "  RNNT_checkpoint_path = checkpoint_dir + \"/cp-{epoch}-rnnt.tf\"\n",
        "   \n",
        "  os.makedirs(current_dir, exist_ok=True)\n",
        "  \n",
        "  try:\n",
        "    text_model.load_weights(tf.train.latest_checkpoint(text_checkpoint_path))\n",
        "    audio_model.load_weights(tf.train.latest_checkpoint(audio_checkpoint_path))\n",
        "    joint_model.load_weights(tf.train.latest_checkpoint(joint_checkpoint_path))\n",
        "    \n",
        "    print(bcolors.OKGREEN + \"Resumed from checkpoints\" + bcolors.ENDC)\n",
        "  except:\n",
        "    try:\n",
        "      tmp_model = RNNT(audio_model, text_model, joint_model)\n",
        "      tmp_model.load_weights(RNNT_checkpoint_path.format(epoch=\"bak\"))\n",
        "      \n",
        "      print(bcolors.OKBLUE + \"No checkpoints found. Resuming from backup joint model\" + bcolors.ENDC)\n",
        "    except:\n",
        "      print(bcolors.WARNING + \"No checkpoints or backups found. Generating fresh weights\" + bcolors.ENDC)\n",
        "  \n",
        "  if True: # TODO: Add proper handling for debug messages\n",
        "    model = RNNT(audio_model, text_model, joint_model) # TODO: Add input_specs so that it doesn't require fit to summarise\n",
        "    model.compile(loss = loss,\n",
        "              optimizer = 'nadam')\n",
        "    model.fit(dataset.take(1))\n",
        "    model.summary()\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    os.makedirs(current_dir, exist_ok=True)\n",
        "    \n",
        "    text_model.save_weights(text_checkpoint_path.format(batch=0), overwrite=True)\n",
        "    audio_model.save_weights(audio_checkpoint_path.format(batch=0), overwrite=True)\n",
        "    joint_model.save_weights(joint_checkpoint_path.format(batch=0), overwrite=True)\n",
        "    \n",
        "    # Text Training\n",
        "    encap_model = EncapsulationModel(text_model, label_size)\n",
        "    \n",
        "    cp_callback= tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=text_checkpoint_path,\n",
        "        verbose=0,\n",
        "        save_weights_only=True,\n",
        "        save_freq=100)\n",
        "    \n",
        "    encap_model.compile(loss='categorical_crossentropy', \n",
        "                        optimizer='nadam', \n",
        "                        metrics=['categorical_accuracy'])\n",
        "    \n",
        "    encap_model.fit(text_dataset,\n",
        "                    callbacks=[cp_callback, ],\n",
        "                    epochs=1)\n",
        "  \n",
        "    # Audio_Training\n",
        "    encap_model = EncapsulationModel(audio_model, label_size, False)\n",
        "\n",
        "    cp_callback= tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=audio_checkpoint_path,\n",
        "        verbose=0,\n",
        "        save_weights_only=True,\n",
        "        save_freq=100)\n",
        "    \n",
        "    encap_model.compile(loss=ctc_loss,\n",
        "                        optimizer='nadam')\n",
        "    encap_model.fit(audio_dataset,\n",
        "                    callbacks=[cp_callback,], \n",
        "                    epochs=1)\n",
        "\n",
        "    encap_model = None\n",
        "\n",
        "    # Regular RNNT Training\n",
        "    model = RNNT(audio_model, text_model, joint_model)\n",
        "\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=joint_checkpoint_path, \n",
        "        verbose=0,\n",
        "        save_weights_only=True,\n",
        "        save_freq=100)\n",
        "\n",
        "    test_callback = TestCallback(\n",
        "        model = model,\n",
        "        dataset = test_dataset\n",
        "    )\n",
        "\n",
        "    model.compile(loss = loss,\n",
        "                  optimizer = 'nadam')\n",
        "\n",
        "    model.fit(dataset,\n",
        "              callbacks=[cp_callback, test_callback], \n",
        "              epochs=1)\n",
        "\n",
        "    model.save_weights(RNNT_checkpoint_path.format(epoch=\"bak\"), overwrite=True)\n",
        "    model.save_weights(RNNT_checkpoint_path.format(epoch=epoch+1))\n",
        "    shutil.rmtree(current_dir, ignore_errors=True)\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "\u001b[33mNo checkpoints or backups found. Generating fresh weights\u001b[0m\n",
            "WARNING:tensorflow:From /tensorflow-2.0.0-rc1/python3.6/tensorflow_core/python/keras/backend.py:3985: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "1/1 [==============================] - 51s 51s/step - loss: 347.7794\n",
            "Model: \"rnnt_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential_1 (Sequential)    multiple                  63946752  \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      multiple                  38066176  \n",
            "_________________________________________________________________\n",
            "joint_model (JointModel)     multiple                  4765254   \n",
            "=================================================================\n",
            "Total params: 106,778,182\n",
            "Trainable params: 106,778,182\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "90/90 [==============================] - 4547s 51s/step - loss: 0.6370 - categorical_accuracy: 0.8464\n",
            "WARNING:tensorflow:From /tensorflow-2.0.0-rc1/python3.6/tensorflow_core/python/ops/ctc_ops.py:1154: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n",
            "WARNING:tensorflow:From /tensorflow-2.0.0-rc1/python3.6/tensorflow_core/python/ops/ctc_ops.py:1137: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n",
            "717/717 [==============================] - 7843s 11s/step - loss: 440.9102\n",
            "    100/Unknown - 504s 5s/step - loss: 337.2027\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/1024 [00:00<00:01, 802.28it/s, Prediction:  (0.00)]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "    100/Unknown - 520s 5s/step - loss: 337.2027"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}